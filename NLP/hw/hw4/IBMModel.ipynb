{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 问题引入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采用传统的机器翻译方法时，手工编制一套双语词典及翻译规则是十分困难的。在统计机器翻译出现以后，我们采用的方法是从大量的平行语料或双语语料中获取翻译知识。基于语料的机器翻译首先需要根据一些对齐规则进行句子级的对齐(sentence alignment)。下面将对翻译模型和句子对齐问题进行更深入的讨论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 计算模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 噪声信道模型\n",
    "假设源语言句子是由某个目标语言的句子经过噪声信道传播得到，使用贝叶斯方法就可以找到最可能产生该源语言句子的目标语言句子，及将源语言句子$f=f_1,f_2,\\cdots,f_m$翻译到目标语言句子$e=e_1,e_2,\\cdots,e_l$使得$P(E|F)$最大化：\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{e}&=argmax_{e\\in English} P(e|f) \\\\\n",
    "        &=argmax_{e\\in English} \\frac{P(f|e)P(e)}{P(f)} \\\\\n",
    "        &=argmax_{e\\in English} P(f|e)P(e) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "噪声信道模型的主要由3部分构成：翻译模型，语言模型和解码器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 语言模型\n",
    "可以采用n-gram语言模型计算$P(e)$，也可以采用较复杂的PCFG语言模型来捕获长距离相依特性来计算$P(e)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 翻译模型-IBM Model 1\n",
    "从英语句子$e$生成一个外文句子$f$:\n",
    "\n",
    "1.依据概率$\\frac{1}{(l+1)^m}$挑选一种对齐方式。\n",
    "\n",
    "2.依据下列概率选择外文句子：\n",
    "$$\n",
    "p(f|a,e,m)=\\prod_{j=1}^m t(f_j|e_{a_j})\n",
    "$$\n",
    "则：\n",
    "$$\n",
    "p(f,a|e,m)=\\frac{1}{(l+1)^m}\\prod_{j=1}^m t(f_j|e_{a_j})\n",
    "$$\n",
    "最后得到：\n",
    "$$\n",
    "p(f|e,m)=\\sum_{a\\in A}p(f,a|e,m)\n",
    "$$\n",
    "\n",
    "3.对于给定的$(f,e)$对，可以计算某种对齐$a$的概率：\n",
    "$$\n",
    "p(a|f,e,m)=\\frac{p(a|e,m)p(f|a,e,m)}{\\sum_{a\\in A}p(f,a|e,m)}\n",
    "$$\n",
    "\n",
    "4.进而，最可能的对齐方式为：\n",
    "$$\n",
    "a^*=argmax_a p(a|f,e,m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 翻译模型-IBM Model 2\n",
    "第二种模型与第一种模型的主要区别就是引入了对齐时扭曲系数：$q(i|j,l,m)$，表示给定$e$和$f$的长度分别为$l$和$m$时，第$j$个外文次和第$i$个英文词对齐的概率。下面是从英语句子$e$生成一个外文句子$f$的过程:\n",
    "\n",
    "1.依据如下概率选择一种对齐方式：$a=\\{a_1,a_2,\\cdots,a_m\\},\\prod_{j=1}^m q(a_j|j,l,m)$\n",
    "\n",
    "2.依据如下概率选择一个外文句子$f$：\n",
    "$$\n",
    "p(f|a,e,m)=\\prod_{j=1}^m t(f_j|e_{a_j})\n",
    "$$\n",
    "进而得到：\n",
    "$$\n",
    "p(f,a|e,m)=\\prod_{j=1}^m q(a_j|j,l,m)t(f_j|e_{a_j})\n",
    "$$\n",
    "最后得到：\n",
    "$$\n",
    "p(f|e,m)=\\sum_{a\\in A}p(f,a|e,m)\n",
    "$$\n",
    "\n",
    "3.如果已经得到参数$q$和$t$，则对于每个句对$e_1,e_2,\\cdots,e_l,f_1,f_2,\\cdots,f_m$，其最优对齐$a_j$为：\n",
    "$$\n",
    "a_j=argmax_{a\\in\\{0,\\cdots,l\\}} q(a|k,l,m)t(f_j|e_a)\n",
    "$$\n",
    "\n",
    "但是，如果我们的训练语料仅包含英文句子和外文句子，而不包含对齐方式，我们需要使用EM算法来估计参数。\n",
    "\n",
    "1.通过迭代计算模型参数$q$和$t$。从一个初始值出发，每次迭代时根据训练数据和当时的$q,t$计算counts，再依据当前的counts重新估计$q,t$。\n",
    "\n",
    "2.每次迭代时，依据下式计算$\\delta(k,i,j)$:\n",
    "$$\n",
    "\\delta(k,i,j)=\\frac{q(j|i,l_k,m_k)t(f_i^{(k)}|e_j^{(k)})}{\\sum_{j=0}^{l_k}q(j|i,l_k,m_k)t(f_i^{(k)}|e_j^{(k)})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 编程实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8-sig\n",
    "import codecs\n",
    "import string\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IBMModel:\n",
    "    def __init__(self, opt=2, num=100000):\n",
    "        self.data,self.en,self.cn = self.load_data(num)\n",
    "        self.t = self.init_t()\n",
    "        self.q= self.init_q()\n",
    "        self.conditional_dict = defaultdict(list)\n",
    "        self.opt = opt\n",
    "\n",
    "    # load txt data，map from en sentences to cn sentences\n",
    "    def load_data(self, num=100000):\n",
    "        print(\"Start loading data...\")\n",
    "        en = []\n",
    "        cn = []\n",
    "        file_list = []\n",
    "        file_list.append(codecs.open('en.txt', \"r\", \"utf-8\"))\n",
    "        file_list.append(codecs.open('cn.txt', \"r\", \"utf-8\"))\n",
    "        i = 0\n",
    "        data = {}\n",
    "        while i < num:\n",
    "            sentence_en = word_tokenize(\"NULL \" + file_list[0].readline().strip(\"\\n\").lower())\n",
    "            sentence_cn = word_tokenize(\"NULL \" + file_list[1].readline().strip(\"\\n\").lower())\n",
    "            sentence_en = [s for s in sentence_en if not s in string.punctuation]\n",
    "            sentence_cn = [s for s in sentence_cn if not s in string.punctuation]\n",
    "            if i == 0:\n",
    "                lyh = sentence_en[1]\n",
    "                ryh = sentence_en[23]\n",
    "            sentence_en = [s for s in sentence_en if not s == lyh]\n",
    "            sentence_cn = [s for s in sentence_cn if not s == lyh]\n",
    "            sentence_en = [s for s in sentence_en if not s == ryh]\n",
    "            sentence_cn = [s for s in sentence_cn if not s == ryh]\n",
    "            sentence_en = tuple(sentence_en)\n",
    "            sentence_cn = tuple(sentence_cn)\n",
    "            data[sentence_en] = sentence_cn\n",
    "            en.append(sentence_en)\n",
    "            cn.append(sentence_cn)\n",
    "            i += 1\n",
    "        print(\"Finish loading data!\")\n",
    "        return data, en, cn\n",
    "\n",
    "    # init t\n",
    "    def init_t(self):\n",
    "        num_of_words = len(set(f_word for (english_sent, foreign_sent) in self.data.items() for f_word in foreign_sent))\n",
    "        t = defaultdict(lambda: float(1 / num_of_words))\n",
    "        return t\n",
    "    # init distortion parameter\n",
    "    def init_q(self):\n",
    "        q = defaultdict(lambda: float(1/100))\n",
    "        return q\n",
    "\n",
    "    def fit(self, max_iter=5):\n",
    "        print(\"Start fitting...\")\n",
    "        for n in range(max_iter):\n",
    "            count_e_given_f = defaultdict(float)\n",
    "            count_i_give_j = defaultdict(float)\n",
    "            qtotal = defaultdict(float)\n",
    "            total = defaultdict(float)\n",
    "            sentence_total = defaultdict(float)\n",
    "            for o in range(len(self.en)):\n",
    "                english_sent=self.en[o]\n",
    "                foreign_sent=self.cn[o]\n",
    "                l1 = len(english_sent)\n",
    "                l2 = len(foreign_sent)\n",
    "                for i in range(len(foreign_sent)):\n",
    "                    for j in range(len(english_sent)):\n",
    "                        if self.opt == 1:\n",
    "                            self.q[(j,i,l1,l2)] = 1/(l1+1)**l2\n",
    "                        sentence_total[(i,l1,l2)] += self.q[(j,i,l1,l2)]*self.t[(foreign_sent[i],english_sent[j])]\n",
    "                for i in range(len(foreign_sent)):\n",
    "                    for j in range(len(english_sent)):\n",
    "                        delta = self.t[(j,i,l1,l2)]*self.t[(foreign_sent[i],english_sent[j])]/sentence_total[(i,l1,l2)]\n",
    "                        count_e_given_f[(foreign_sent[i],english_sent[j])] += delta\n",
    "                        total[(english_sent[j])] += delta\n",
    "                        count_i_give_j[(i,j,l1,l2)] += delta\n",
    "                        total[(i,l1,l2)] += delta\n",
    "                for i in range(len(foreign_sent)):\n",
    "                    for j in range(len(english_sent)):\n",
    "                        self.t[(foreign_sent[i],english_sent[j])] = count_e_given_f[(foreign_sent[i],english_sent[j])]/total[(english_sent[j])]\n",
    "                        if self.opt == 2:\n",
    "                            self.q[(j,i,l1,l2)] = count_i_give_j[(i,j,l1,l2)]/total[(i,l1,l2)]\n",
    "            print(\"iter = \" + str(n))\n",
    "        print(\"Finish fitting!\")\n",
    "\n",
    "    # find the best alignments\n",
    "    def get_alignments(self):\n",
    "        a = []\n",
    "        for t in range(len(self.en)):\n",
    "            english_sent = self.en[t]\n",
    "            foreign_sent = self.cn[t]\n",
    "            a.append([])\n",
    "            for k in range(len(foreign_sent)):\n",
    "                a[t].append(0)\n",
    "            for i in range(len(foreign_sent)):\n",
    "                p = 0\n",
    "                for j in range(len(english_sent)):\n",
    "                    if self.t[(foreign_sent[i], english_sent[j])]*self.q[(j,i,len(english_sent),len(foreign_sent))] > p:\n",
    "                        p = self.t[(foreign_sent[i], english_sent[j])]*self.q[(j,i,len(english_sent),len(foreign_sent))]\n",
    "                        a[t][i] = j\n",
    "        return a\n",
    "\n",
    "    def print_t(self, max_iter=30):\n",
    "        iterations = 0\n",
    "        for ((f_word, e_word), value) in sorted(self.t.items(), key=itemgetter(1), reverse=True):\n",
    "            if iterations < max_iter:\n",
    "                print(\"{}, {}\".format(\"t(%s|%s)\" % (f_word, e_word), value))\n",
    "            else:\n",
    "                break\n",
    "            iterations += 1\n",
    "        i= 0\n",
    "#         for m in self.q.keys():\n",
    "#             if i<600:\n",
    "#                 print(m,self.q[m])\n",
    "#                 i+=1\n",
    "#             else:\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading data...\n",
      "Finish loading data!\n",
      "Start fitting...\n",
      "iter = 0\n",
      "iter = 1\n",
      "iter = 2\n",
      "iter = 3\n",
      "iter = 4\n",
      "Finish fitting!\n"
     ]
    }
   ],
   "source": [
    "ibm_1 = IBMModel(1)\n",
    "ibm_1.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t(--|happy), 0.9999999996852461\n",
      "t(倒杯水|again), 0.9999999992319613\n",
      "t(一月|often), 0.9999999973003395\n",
      "t(销售部|please), 0.9999999876060397\n",
      "t(问|alone), 0.9999999862277923\n",
      "t(鞋子|heavy), 0.9999998843037965\n",
      "t(灵便|shoulder), 0.9999997839828911\n",
      "t(根由|source), 0.9999996961339709\n",
      "t(有效|off), 0.9999989015575106\n",
      "t(的|unfavorable), 0.9999988992183628\n",
      "t(的|recurrence), 0.9999977666862465\n",
      "t(海军|admiral), 0.9999976739504861\n",
      "t(车门|all), 0.999997018636993\n",
      "t(的|up-to-date), 0.9999969996832432\n",
      "t(的|jumbled), 0.9999968611012439\n",
      "t(斑马|zebra), 0.9999967178870982\n",
      "t(利|small), 0.999996421746315\n",
      "t(的|contingent), 0.9999963125579988\n",
      "t(她说|always), 0.9999955120051789\n",
      "t(，|rallied), 0.9999945217732751\n",
      "t(，|begged), 0.9999942232334081\n",
      "t(的|dwelt), 0.9999937563725585\n",
      "t(的|rash), 0.9999928766103096\n",
      "t(犯错误|all), 0.9999921090346664\n",
      "t(的|unseen), 0.9999905415153532\n",
      "t(的|copyright), 0.9999902584718418\n",
      "t(的|sadness), 0.9999902425725843\n",
      "t(明白地|door), 0.9999899167350783\n",
      "t(的|instilled), 0.9999897176967448\n",
      "t(摩尔多瓦共和国|moldova), 0.9999891268364945\n",
      "[[18, 9, 9, 8, 15, 11, 13, 20, 16, 14, 13, 17, 8, 2, 8, 19, 5, 15, 16, 14, 21, 4], [1, 7, 0, 7, 6, 2, 7, 0, 7, 3, 2, 7, 10, 7, 7, 4], [2, 5, 13, 8, 1, 6, 10, 12, 10, 1, 1, 6, 11], [3, 8, 7, 10, 7, 11, 12, 12, 2], [7, 2, 3, 11, 13, 5, 8, 11, 14, 11, 14, 9, 8, 8, 0, 1, 6], [7, 14, 9, 4, 13, 13, 3, 0, 7, 13, 5, 10, 6, 3, 13, 1], [5, 8, 11, 8, 11, 3, 9, 7, 0, 6, 10, 6, 3, 4], [2, 8, 9, 6, 5, 12, 10, 10, 5, 10, 4, 5, 3], [2, 3, 12, 9, 8, 13, 7, 6, 9, 5, 8, 6, 9, 4], [11, 4, 4, 2, 11, 22, 17, 24, 2, 2, 6, 9, 3, 1, 15, 7, 0, 7, 24, 26, 7, 7, 24, 23, 10, 3, 7, 10], [5, 2, 8, 4, 8, 9, 9, 6, 8, 6, 9], [2, 13, 5, 0, 10, 6, 10, 4, 12, 1, 4, 5, 8, 5, 10, 14, 2], [10, 8, 16, 16, 6, 2, 10, 2, 4, 14, 9, 9, 11, 16, 16, 6, 2, 10, 13, 16, 6, 9, 9, 16, 3], [4, 6, 6, 6, 15, 5, 14, 8, 13, 3, 12, 0, 12, 6, 3, 5, 5, 5, 8, 10, 1, 1, 11, 6, 7, 9, 1], [6, 10, 7, 7, 5, 1, 4, 9, 7, 7, 4, 9, 9, 9, 9, 12], [3, 8, 8, 12, 1, 17, 2, 12, 12, 6, 8, 3, 5, 11, 16, 16, 11, 3, 2, 4], [8, 3, 5, 4, 9, 9, 4, 5, 3, 5, 6, 7], [5, 3, 3, 4, 10, 3, 2, 11, 0, 11, 3, 7], [5, 2, 1, 2, 1, 2, 7, 1, 3, 4, 3, 1], [12, 2, 5, 8, 14, 0, 5, 8, 11, 4, 5, 8, 8, 5, 2], [4, 1, 2, 8, 1, 8], [5, 10, 4, 10, 4, 10, 10, 10, 6, 10, 10, 10], [5, 4, 10, 10, 10, 10, 10, 10, 10], [5, 2, 4, 3, 4, 4, 3, 6, 5], [9, 8, 18, 19, 19, 14, 16, 2, 2, 17, 6, 5, 5, 4, 17, 8, 11, 20, 15, 10, 14, 19, 8, 19, 12, 14, 11, 6, 4], [11, 14, 12, 12, 20, 8, 22, 7, 12, 22, 12, 6, 7, 18, 12, 10, 7, 14], [3, 10, 2, 4, 14, 4, 8, 2, 4, 16, 7, 10, 15, 16, 4, 5, 4], [3, 10, 5, 10, 4, 4, 2, 11, 10, 6, 7, 3, 9, 11, 9, 2, 5, 5, 7], [3, 7, 13, 13, 11, 11, 4, 13, 4, 8, 7, 9, 11, 4, 8], [13, 6, 18, 7, 15, 3, 9, 10, 6, 2, 9, 14, 9, 11, 11, 4, 15, 6, 14, 12, 17, 12], [9, 12, 8, 1, 0, 3, 4, 11, 0, 6, 8, 3, 10, 4, 4, 4, 6, 7, 12], [1, 2, 4, 3, 4, 3, 4, 4, 3], [13, 16, 11, 14, 26, 6, 21, 26, 26, 14, 26, 2, 16, 11, 22, 7, 11, 22, 8, 4, 20, 3, 17, 21, 2], [7, 3, 5, 3, 3, 8, 3, 3, 5, 8, 7, 2], [11, 7, 7, 9, 7, 7, 7, 0, 5, 4, 4, 2], [1, 5, 8, 6, 9, 8, 8, 9, 8, 10, 8], [1, 14, 20, 17, 4, 14, 14, 16, 7, 9, 20, 7, 14, 3, 14, 3, 13, 0, 4, 14, 3, 14, 6, 4, 5], [1, 4, 5, 6, 6, 5, 6, 6, 6, 5, 5, 2], [8, 7, 3, 7, 10, 6, 13, 11, 7, 4, 9, 9, 9, 9, 4], [11, 3, 6, 9, 7, 3, 6, 3, 3, 4, 8], [3, 6, 6, 5, 4, 2, 9, 6, 9, 10], [11, 12, 8, 13, 10, 13, 4, 5, 9, 5, 2, 12, 2, 4], [2, 8, 12, 12, 16, 10, 3, 12, 5, 4, 3, 16, 15, 10], [6, 4, 4, 7, 2, 5], [5, 8, 7, 7, 9, 9, 3, 9, 3, 9, 2], [2, 13, 4, 3, 11, 5, 4, 1, 11, 11, 11, 6], [5, 6, 2, 0, 4, 2, 1], [5, 8, 3, 11, 17, 10, 4, 2, 14, 14, 0, 1, 12, 9, 15, 2, 2, 14, 6, 2, 14, 10, 7], [2, 3, 10, 5, 5, 6, 0, 9, 11, 6, 4, 12, 1], [7, 8, 9, 11, 13, 6, 13, 9, 2, 1, 5, 1, 11, 3, 1, 9, 1, 4]]\n"
     ]
    }
   ],
   "source": [
    "ibm_1.print_t()\n",
    "a = ibm_1.get_alignments()\n",
    "print(a[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading data...\n",
      "Finish loading data!\n",
      "Start fitting...\n",
      "iter = 0\n",
      "iter = 1\n",
      "iter = 2\n",
      "iter = 3\n",
      "iter = 4\n",
      "Finish fitting!\n"
     ]
    }
   ],
   "source": [
    "ibm_2 = IBMModel(2)\n",
    "ibm_2.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t(摩尔多瓦共和国|moldova), 0.9999992948607731\n",
      "t(布隆迪共和国|burundi), 0.9999989149897981\n",
      "t(爱沙尼亚共和国|estonia), 0.9999938253291653\n",
      "t(波特率|baud), 0.9999799966783909\n",
      "t(斑马|zebra), 0.9999798580057734\n",
      "t(几内亚比绍共和国|guinea-bissau), 0.9999765154982379\n",
      "t(马尔代夫共和国|maldives), 0.9999624478024018\n",
      "t(斯|jones), 0.9999511356807737\n",
      "t(基因图谱|genetic), 0.9999430221353006\n",
      "t(巴布亚新几内亚|papua), 0.9999400384390129\n",
      "t(巴布亚新几内亚|gunea), 0.9999400384390129\n",
      "t(急|written), 0.999927675688751\n",
      "t(毛里求斯共和国|mauritius), 0.9999206859579925\n",
      "t(黎巴嫩共和国|lebanese), 0.9999193731255746\n",
      "t(瓦努阿图共和国|vanuatu), 0.9999140007667047\n",
      "t(卢旺达共和国|rwandese), 0.9998857631853852\n",
      "t(忽然|abruptly), 0.9998837040663799\n",
      "t(就在|telephone), 0.999879844550907\n",
      "t(太远|furthermore), 0.9998769598829513\n",
      "t(出门|rolled), 0.9998616861289754\n",
      "t(赞比亚共和国|zambia), 0.9998575905864929\n",
      "t(成吉思汗|genghis), 0.9998485568210402\n",
      "t(斯洛伐克共和国|slovak), 0.9998437234642802\n",
      "t(海军|naval), 0.9998311515032571\n",
      "t(计算机病毒|virus), 0.9998226789267802\n",
      "t(越南社会主义共和国|viet), 0.9998064210615605\n",
      "t(越南社会主义共和国|nam), 0.9998064210615605\n",
      "t(冷|far), 0.9997965679964608\n",
      "t(道歉|apology), 0.9997489020404325\n",
      "t(斯里兰卡民主社会主义共和国|sri), 0.9997137646434846\n",
      "[[0, 1, 9, 17, 15, 19, 13, 20, 16, 10, 9, 12, 8, 8, 11, 13, 5, 15, 16, 1, 21, 0], [0, 2, 5, 3, 9, 7, 10, 1, 6, 11, 2, 9, 10, 10, 3, 0], [0, 1, 4, 12, 3, 6, 9, 12, 8, 7, 10, 9, 8], [0, 1, 6, 8, 2, 11, 9, 12, 0], [0, 2, 1, 11, 13, 9, 8, 5, 5, 11, 4, 11, 8, 8, 7, 4, 0], [0, 1, 9, 4, 13, 13, 6, 8, 7, 13, 7, 10, 11, 9, 1, 0], [0, 9, 11, 8, 11, 3, 10, 7, 10, 6, 1, 6, 1, 0], [0, 3, 1, 5, 12, 12, 10, 9, 5, 9, 12, 5, 0], [0, 3, 12, 1, 8, 13, 7, 1, 13, 5, 10, 6, 1, 0], [0, 17, 4, 23, 3, 5, 5, 10, 23, 2, 6, 9, 8, 25, 15, 15, 16, 3, 14, 17, 19, 20, 10, 21, 10, 3, 23, 0], [0, 2, 4, 1, 3, 6, 9, 2, 1, 8, 0], [0, 13, 8, 1, 5, 6, 7, 11, 8, 10, 4, 12, 8, 8, 13, 14, 0], [0, 8, 16, 6, 15, 2, 5, 3, 12, 14, 15, 7, 11, 16, 15, 15, 2, 5, 13, 16, 14, 15, 16, 15, 0], [0, 6, 10, 10, 15, 8, 14, 7, 2, 3, 12, 13, 12, 9, 4, 8, 5, 6, 14, 6, 1, 1, 11, 2, 7, 5, 1], [0, 2, 7, 7, 5, 16, 1, 5, 7, 7, 1, 10, 13, 15, 9, 0], [0, 6, 8, 12, 1, 2, 2, 12, 12, 13, 8, 3, 15, 11, 16, 16, 11, 10, 2, 4], [0, 3, 4, 1, 6, 6, 1, 5, 3, 5, 9, 0], [0, 3, 3, 4, 2, 3, 10, 11, 5, 11, 3, 0], [0, 2, 1, 2, 1, 2, 7, 1, 3, 4, 3, 0], [0, 2, 5, 8, 1, 13, 8, 5, 11, 4, 5, 9, 13, 5, 0], [0, 1, 2, 8, 1, 8], [0, 9, 4, 2, 4, 7, 10, 10, 7, 6, 4, 7], [0, 4, 10, 8, 10, 10, 10, 10, 6], [0, 2, 5, 3, 4, 4, 3, 6, 0], [0, 11, 6, 6, 10, 17, 5, 5, 6, 8, 6, 17, 5, 13, 20, 11, 11, 3, 15, 10, 18, 12, 12, 19, 19, 19, 11, 13, 13], [0, 7, 12, 15, 16, 8, 15, 18, 12, 17, 15, 10, 14, 20, 12, 9, 22, 2], [0, 6, 2, 4, 14, 11, 8, 2, 9, 16, 5, 13, 15, 16, 17, 5, 11], [0, 1, 10, 2, 2, 8, 4, 11, 4, 3, 9, 5, 10, 11, 11, 2, 10, 5, 0], [0, 7, 13, 13, 11, 2, 9, 13, 9, 4, 7, 9, 8, 4, 0], [0, 1, 2, 2, 13, 16, 8, 4, 6, 9, 11, 14, 8, 14, 14, 4, 13, 17, 9, 16, 3, 0], [0, 5, 8, 4, 2, 3, 3, 11, 12, 6, 3, 3, 8, 11, 11, 7, 10, 7, 0], [0, 2, 4, 1, 2, 3, 3, 4, 2], [0, 7, 4, 2, 7, 6, 21, 26, 14, 12, 14, 24, 14, 4, 25, 7, 4, 25, 15, 21, 20, 3, 17, 21, 0], [0, 5, 2, 4, 3, 8, 3, 6, 7, 8, 5, 0], [0, 2, 7, 4, 3, 2, 6, 10, 8, 4, 4, 0], [0, 2, 3, 9, 5, 8, 5, 9, 10, 8, 8], [0, 2, 17, 17, 16, 3, 2, 8, 18, 9, 17, 12, 6, 16, 14, 3, 16, 15, 11, 21, 11, 20, 13, 16, 0], [0, 4, 5, 6, 5, 5, 2, 6, 6, 3, 1, 0], [0, 7, 4, 7, 9, 9, 2, 1, 7, 13, 10, 10, 3, 3, 2], [0, 3, 3, 9, 10, 5, 6, 9, 12, 4, 0], [0, 6, 6, 5, 12, 4, 10, 6, 9, 0], [0, 14, 8, 15, 10, 13, 7, 5, 1, 13, 2, 12, 8, 9], [0, 8, 8, 8, 4, 1, 8, 15, 15, 5, 3, 4, 12, 0], [0, 7, 4, 5, 2, 0], [0, 8, 4, 3, 1, 9, 1, 4, 3, 7, 0], [0, 6, 4, 11, 11, 5, 13, 1, 11, 11, 11, 0], [0, 6, 7, 5, 4, 2, 0], [0, 23, 16, 4, 17, 10, 4, 2, 9, 23, 10, 10, 21, 9, 15, 17, 2, 14, 6, 17, 14, 13, 0], [0, 6, 12, 5, 5, 6, 2, 9, 10, 12, 6, 10, 0], [0, 13, 9, 10, 3, 6, 13, 9, 2, 8, 2, 13, 10, 9, 1, 9, 1, 4]]\n"
     ]
    }
   ],
   "source": [
    "ibm_2.print_t()\n",
    "a = ibm_2.get_alignments()\n",
    "print(a[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 模型评估"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
